{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perceptron with data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFihAePyQHah"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqL1J3eprNgI"
      },
      "source": [
        "#libraries needed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from math import exp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_S-SgpuQK34"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOr0ZnwHwgY7",
        "outputId": "ca8bf119-9370-476a-a0bf-e687aa067a93"
      },
      "source": [
        "!git clone https://github.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection.git\n",
        "df = pd.read_csv(\"Kaggle-Data-Credit-Card-Fraud-Detection/creditcard.csv\")\n",
        "#df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Kaggle-Data-Credit-Card-Fraud-Detection'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 8\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ26M2udXHqU"
      },
      "source": [
        "df_s = df.groupby('Class', group_keys=False).apply(lambda x: x.sample(int(np.rint(2000*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "R0PIKpWWbOh-",
        "outputId": "1201a038-74f8-4063-e90e-fa10385b0dcc"
      },
      "source": [
        "df_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34428.0</td>\n",
              "      <td>-0.732564</td>\n",
              "      <td>1.161535</td>\n",
              "      <td>0.969053</td>\n",
              "      <td>0.776670</td>\n",
              "      <td>0.479550</td>\n",
              "      <td>1.023593</td>\n",
              "      <td>0.110560</td>\n",
              "      <td>0.836507</td>\n",
              "      <td>-0.869948</td>\n",
              "      <td>-0.630049</td>\n",
              "      <td>0.575756</td>\n",
              "      <td>0.514433</td>\n",
              "      <td>-0.046908</td>\n",
              "      <td>0.799291</td>\n",
              "      <td>2.035625</td>\n",
              "      <td>-1.567332</td>\n",
              "      <td>1.303327</td>\n",
              "      <td>-1.905038</td>\n",
              "      <td>-1.245304</td>\n",
              "      <td>-0.277786</td>\n",
              "      <td>0.336932</td>\n",
              "      <td>1.056984</td>\n",
              "      <td>0.039673</td>\n",
              "      <td>-0.618691</td>\n",
              "      <td>-0.594717</td>\n",
              "      <td>-0.224113</td>\n",
              "      <td>0.122782</td>\n",
              "      <td>0.090079</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36687.0</td>\n",
              "      <td>-0.346766</td>\n",
              "      <td>1.176695</td>\n",
              "      <td>1.305150</td>\n",
              "      <td>0.057486</td>\n",
              "      <td>0.084744</td>\n",
              "      <td>-0.960084</td>\n",
              "      <td>0.776882</td>\n",
              "      <td>-0.117719</td>\n",
              "      <td>-0.511599</td>\n",
              "      <td>-0.537243</td>\n",
              "      <td>-0.081468</td>\n",
              "      <td>0.323776</td>\n",
              "      <td>0.877522</td>\n",
              "      <td>-0.579962</td>\n",
              "      <td>0.810737</td>\n",
              "      <td>0.347505</td>\n",
              "      <td>-0.034990</td>\n",
              "      <td>-0.230513</td>\n",
              "      <td>-0.097366</td>\n",
              "      <td>0.170587</td>\n",
              "      <td>-0.252797</td>\n",
              "      <td>-0.627172</td>\n",
              "      <td>-0.015071</td>\n",
              "      <td>0.354593</td>\n",
              "      <td>-0.133454</td>\n",
              "      <td>0.069188</td>\n",
              "      <td>0.249306</td>\n",
              "      <td>0.100198</td>\n",
              "      <td>6.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41508.0</td>\n",
              "      <td>1.477548</td>\n",
              "      <td>-1.111769</td>\n",
              "      <td>0.475837</td>\n",
              "      <td>-1.425269</td>\n",
              "      <td>-1.524183</td>\n",
              "      <td>-0.631358</td>\n",
              "      <td>-1.059259</td>\n",
              "      <td>-0.142672</td>\n",
              "      <td>-1.679893</td>\n",
              "      <td>1.406274</td>\n",
              "      <td>-0.904954</td>\n",
              "      <td>-1.107463</td>\n",
              "      <td>0.430666</td>\n",
              "      <td>-0.365090</td>\n",
              "      <td>0.815266</td>\n",
              "      <td>-0.030655</td>\n",
              "      <td>0.256578</td>\n",
              "      <td>0.107768</td>\n",
              "      <td>-0.060401</td>\n",
              "      <td>-0.257542</td>\n",
              "      <td>-0.385084</td>\n",
              "      <td>-0.833469</td>\n",
              "      <td>0.092679</td>\n",
              "      <td>-0.176714</td>\n",
              "      <td>0.179861</td>\n",
              "      <td>-0.381506</td>\n",
              "      <td>0.035379</td>\n",
              "      <td>0.029673</td>\n",
              "      <td>44.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>108931.0</td>\n",
              "      <td>-1.232699</td>\n",
              "      <td>0.703833</td>\n",
              "      <td>1.670609</td>\n",
              "      <td>-1.061069</td>\n",
              "      <td>0.781721</td>\n",
              "      <td>2.690038</td>\n",
              "      <td>-0.416013</td>\n",
              "      <td>0.685872</td>\n",
              "      <td>2.810218</td>\n",
              "      <td>-0.080042</td>\n",
              "      <td>1.360014</td>\n",
              "      <td>-1.230436</td>\n",
              "      <td>2.308869</td>\n",
              "      <td>0.125871</td>\n",
              "      <td>-1.755239</td>\n",
              "      <td>-0.643893</td>\n",
              "      <td>0.649979</td>\n",
              "      <td>-0.631734</td>\n",
              "      <td>-0.793737</td>\n",
              "      <td>0.420916</td>\n",
              "      <td>-0.259754</td>\n",
              "      <td>0.401432</td>\n",
              "      <td>-0.060176</td>\n",
              "      <td>-0.947597</td>\n",
              "      <td>-0.479991</td>\n",
              "      <td>0.539294</td>\n",
              "      <td>0.567106</td>\n",
              "      <td>0.165219</td>\n",
              "      <td>2.52</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>85207.0</td>\n",
              "      <td>1.175246</td>\n",
              "      <td>0.174460</td>\n",
              "      <td>0.225492</td>\n",
              "      <td>1.092996</td>\n",
              "      <td>-0.221987</td>\n",
              "      <td>-0.526466</td>\n",
              "      <td>0.062496</td>\n",
              "      <td>-0.018342</td>\n",
              "      <td>0.189901</td>\n",
              "      <td>0.006117</td>\n",
              "      <td>-0.410864</td>\n",
              "      <td>-0.472257</td>\n",
              "      <td>-1.423752</td>\n",
              "      <td>0.699816</td>\n",
              "      <td>1.327106</td>\n",
              "      <td>-0.176208</td>\n",
              "      <td>-0.066897</td>\n",
              "      <td>-0.426059</td>\n",
              "      <td>-0.723897</td>\n",
              "      <td>-0.239449</td>\n",
              "      <td>0.046804</td>\n",
              "      <td>0.119416</td>\n",
              "      <td>-0.063903</td>\n",
              "      <td>0.048737</td>\n",
              "      <td>0.562528</td>\n",
              "      <td>-0.295394</td>\n",
              "      <td>0.018616</td>\n",
              "      <td>0.016184</td>\n",
              "      <td>17.24</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>125155.0</td>\n",
              "      <td>2.079955</td>\n",
              "      <td>-0.715066</td>\n",
              "      <td>-0.868426</td>\n",
              "      <td>-0.234694</td>\n",
              "      <td>-0.465139</td>\n",
              "      <td>-0.237229</td>\n",
              "      <td>-0.480820</td>\n",
              "      <td>-0.143365</td>\n",
              "      <td>-0.364310</td>\n",
              "      <td>0.780563</td>\n",
              "      <td>-1.178171</td>\n",
              "      <td>0.526171</td>\n",
              "      <td>1.243597</td>\n",
              "      <td>-0.267311</td>\n",
              "      <td>-0.136682</td>\n",
              "      <td>-1.325943</td>\n",
              "      <td>-0.268281</td>\n",
              "      <td>0.491128</td>\n",
              "      <td>-0.493667</td>\n",
              "      <td>-0.457234</td>\n",
              "      <td>-0.849268</td>\n",
              "      <td>-1.951445</td>\n",
              "      <td>0.552179</td>\n",
              "      <td>0.400118</td>\n",
              "      <td>-0.637778</td>\n",
              "      <td>-0.264562</td>\n",
              "      <td>-0.014489</td>\n",
              "      <td>-0.026507</td>\n",
              "      <td>44.91</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>32399.0</td>\n",
              "      <td>0.824325</td>\n",
              "      <td>-0.624973</td>\n",
              "      <td>1.028627</td>\n",
              "      <td>0.680266</td>\n",
              "      <td>-0.704396</td>\n",
              "      <td>0.928472</td>\n",
              "      <td>-0.666306</td>\n",
              "      <td>0.393218</td>\n",
              "      <td>0.425848</td>\n",
              "      <td>-0.174724</td>\n",
              "      <td>1.638029</td>\n",
              "      <td>1.572414</td>\n",
              "      <td>0.733693</td>\n",
              "      <td>-0.168660</td>\n",
              "      <td>0.383362</td>\n",
              "      <td>-0.086022</td>\n",
              "      <td>-0.057964</td>\n",
              "      <td>-0.439825</td>\n",
              "      <td>-0.740940</td>\n",
              "      <td>0.152703</td>\n",
              "      <td>0.288409</td>\n",
              "      <td>0.781606</td>\n",
              "      <td>-0.128156</td>\n",
              "      <td>-0.204782</td>\n",
              "      <td>0.152404</td>\n",
              "      <td>0.493468</td>\n",
              "      <td>0.023969</td>\n",
              "      <td>0.031453</td>\n",
              "      <td>132.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>20658.0</td>\n",
              "      <td>-1.849175</td>\n",
              "      <td>-1.717241</td>\n",
              "      <td>0.606939</td>\n",
              "      <td>-2.712053</td>\n",
              "      <td>0.231939</td>\n",
              "      <td>-0.608639</td>\n",
              "      <td>-0.675035</td>\n",
              "      <td>0.410203</td>\n",
              "      <td>1.244163</td>\n",
              "      <td>-1.775073</td>\n",
              "      <td>-0.582874</td>\n",
              "      <td>-1.754898</td>\n",
              "      <td>3.139620</td>\n",
              "      <td>1.300657</td>\n",
              "      <td>-0.216583</td>\n",
              "      <td>-1.971676</td>\n",
              "      <td>0.784191</td>\n",
              "      <td>1.492471</td>\n",
              "      <td>-0.704236</td>\n",
              "      <td>0.081933</td>\n",
              "      <td>-0.200378</td>\n",
              "      <td>-0.457460</td>\n",
              "      <td>0.121874</td>\n",
              "      <td>-0.992942</td>\n",
              "      <td>-0.105379</td>\n",
              "      <td>-0.261465</td>\n",
              "      <td>0.018400</td>\n",
              "      <td>-0.155046</td>\n",
              "      <td>134.22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>55407.0</td>\n",
              "      <td>-1.066003</td>\n",
              "      <td>0.672425</td>\n",
              "      <td>1.391318</td>\n",
              "      <td>0.248053</td>\n",
              "      <td>-0.201483</td>\n",
              "      <td>-0.622653</td>\n",
              "      <td>0.497962</td>\n",
              "      <td>0.004466</td>\n",
              "      <td>0.540652</td>\n",
              "      <td>0.001753</td>\n",
              "      <td>-1.051991</td>\n",
              "      <td>-0.193695</td>\n",
              "      <td>-0.809723</td>\n",
              "      <td>-0.286539</td>\n",
              "      <td>-0.532845</td>\n",
              "      <td>-0.478472</td>\n",
              "      <td>0.104936</td>\n",
              "      <td>-0.319012</td>\n",
              "      <td>0.328414</td>\n",
              "      <td>0.059065</td>\n",
              "      <td>-0.076169</td>\n",
              "      <td>0.221602</td>\n",
              "      <td>-0.001778</td>\n",
              "      <td>0.412815</td>\n",
              "      <td>-0.070482</td>\n",
              "      <td>0.366590</td>\n",
              "      <td>0.358369</td>\n",
              "      <td>0.280272</td>\n",
              "      <td>28.49</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>86676.0</td>\n",
              "      <td>-3.071804</td>\n",
              "      <td>2.606792</td>\n",
              "      <td>-1.969949</td>\n",
              "      <td>0.927324</td>\n",
              "      <td>-0.229336</td>\n",
              "      <td>-1.033006</td>\n",
              "      <td>0.241743</td>\n",
              "      <td>1.048155</td>\n",
              "      <td>-0.221141</td>\n",
              "      <td>0.902925</td>\n",
              "      <td>-1.499227</td>\n",
              "      <td>0.680821</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>1.010443</td>\n",
              "      <td>-0.340506</td>\n",
              "      <td>-0.839379</td>\n",
              "      <td>0.591999</td>\n",
              "      <td>-0.356000</td>\n",
              "      <td>0.749363</td>\n",
              "      <td>-0.190987</td>\n",
              "      <td>0.014984</td>\n",
              "      <td>0.620655</td>\n",
              "      <td>0.167745</td>\n",
              "      <td>-0.146739</td>\n",
              "      <td>0.395217</td>\n",
              "      <td>-0.381391</td>\n",
              "      <td>-0.078375</td>\n",
              "      <td>0.020106</td>\n",
              "      <td>25.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0      34428.0 -0.732564  1.161535  0.969053  ...  0.122782  0.090079    1.50      0\n",
              "1      36687.0 -0.346766  1.176695  1.305150  ...  0.249306  0.100198    6.99      0\n",
              "2      41508.0  1.477548 -1.111769  0.475837  ...  0.035379  0.029673   44.00      0\n",
              "3     108931.0 -1.232699  0.703833  1.670609  ...  0.567106  0.165219    2.52      0\n",
              "4      85207.0  1.175246  0.174460  0.225492  ...  0.018616  0.016184   17.24      0\n",
              "...        ...       ...       ...       ...  ...       ...       ...     ...    ...\n",
              "1995  125155.0  2.079955 -0.715066 -0.868426  ... -0.014489 -0.026507   44.91      0\n",
              "1996   32399.0  0.824325 -0.624973  1.028627  ...  0.023969  0.031453  132.00      0\n",
              "1997   20658.0 -1.849175 -1.717241  0.606939  ...  0.018400 -0.155046  134.22      0\n",
              "1998   55407.0 -1.066003  0.672425  1.391318  ...  0.358369  0.280272   28.49      0\n",
              "1999   86676.0 -3.071804  2.606792 -1.969949  ... -0.078375  0.020106   25.77      0\n",
              "\n",
              "[2000 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWazQoyQGXa0"
      },
      "source": [
        "## Class (Sigmoid)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3URcov-grnTW"
      },
      "source": [
        "class Perceptron:\n",
        "  \"\"\"Perceptron class which simulates an artficial neuron, with the help of some activation function\"\"\"\n",
        "  \n",
        "  def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "      self.lr = learning_rate\n",
        "      self.n_iters = n_iters\n",
        "      self.activation_fn = self._sigmoid_function\n",
        "      self.weights = None\n",
        "      self.bias = None\n",
        "\n",
        "    #activation function\n",
        "  def _sigmoid_function(self, x):\n",
        "      return 1/(1+np.exp(-x))\n",
        "\n",
        "      \n",
        "  def fit(self, X, y):\n",
        "      # Dimension of the data\n",
        "      n_samples, n_features = X.shape\n",
        "\n",
        "      # Initialize paramters\n",
        "      self.weights = np.zeros(n_features)\n",
        "      self.bias = 0\n",
        "\n",
        "      # Basic cleaning of data to bring it into 1's and 0's\n",
        "      y_ = [1 if i>0 else 0 for i in y]\n",
        "\n",
        "      # Lets start gradient descenting to calculate the right \n",
        "      for _ in range(self.n_iters):\n",
        "        for idx, x_i in enumerate(X):\n",
        "\n",
        "          lm = np.dot(x_i, self.weights) + self.bias\n",
        "          y_predicted = self.activation_fn(lm)\n",
        "\n",
        "          # Update rule for the Perceptron\n",
        "          update = self.lr * (y_[idx] - y_predicted)\n",
        "\n",
        "          self.weights += update * x_i\n",
        "          self.bias += update\n",
        "\n",
        "  def predict(self, X):\n",
        "      lm = np.dot(X, self.weights) + self.bias\n",
        "      y_predicted = self.activation_fn(lm)\n",
        "      return y_predicted\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4kSXpUOGceg"
      },
      "source": [
        "y = df_s['Class']\n",
        "X = df_s.drop(['Class'], axis = 1)\n",
        "X = X.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVPEDgVWGrgo"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqD-FMkOwPv7",
        "outputId": "10238493-9bb1-4afe-f77d-21ae7ebc56ad"
      },
      "source": [
        "p = Perceptron(learning_rate=0.01)\n",
        "p.fit(X_train, y_train)\n",
        "predicted = p.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in exp\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYamF1gN8QRD",
        "outputId": "b42a07bf-26d4-46cb-d4d4-036b599550b4"
      },
      "source": [
        "accuracy(y_test,predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RVGFHBJzRXp"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzOYnP5aQBmn"
      },
      "source": [
        "## Class (relU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1RmdRcN0w72"
      },
      "source": [
        "class Perceptron:\n",
        "  \"\"\"Perceptron class which simulates an artficial neuron, with the help of some activation function\"\"\"\n",
        "  \n",
        "  def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "      self.lr = learning_rate\n",
        "      self.n_iters = n_iters\n",
        "      self.activation_fn = self.rectified\n",
        "      self.weights = None\n",
        "      self.bias = None\n",
        "\n",
        "    #activation function\n",
        "  def rectified(self,x):\n",
        "    return max(0.0, x.any())\n",
        "\n",
        "      \n",
        "  def fit(self, X, y):\n",
        "      # Dimension of the data\n",
        "      n_samples, n_features = X.shape\n",
        "\n",
        "      # Initialize paramters\n",
        "      self.weights = np.zeros(n_features)\n",
        "      self.bias = 0\n",
        "\n",
        "      # Basic cleaning of data to bring it into 1's and 0's\n",
        "      y_ = [1 if i>0 else 0 for i in y]\n",
        "\n",
        "      # Lets start gradient descenting to calculate the right \n",
        "      for _ in range(self.n_iters):\n",
        "        for idx, x_i in enumerate(X):\n",
        "\n",
        "          lm = np.dot(x_i, self.weights) + self.bias\n",
        "          y_predicted = self.activation_fn(lm)\n",
        "\n",
        "          # Update rule for the Perceptron\n",
        "          update = self.lr * (y_[idx] - y_predicted)\n",
        "\n",
        "          self.weights += update * x_i\n",
        "          self.bias += update\n",
        "\n",
        "  def predict(self, X):\n",
        "      lm = np.dot(X, self.weights) + self.bias\n",
        "      y_predicted = self.activation_fn(lm)\n",
        "      return y_predicted\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTJVNKz9-D2C"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOkkAqIh1HBw"
      },
      "source": [
        "p = Perceptron(learning_rate=0.001)\n",
        "p.fit(X_train, y_train)\n",
        "predicted = p.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ylyfSW-oRs",
        "outputId": "f5230382-9c99-44e1-c83b-4192f6cd721a"
      },
      "source": [
        "accuracy(y_test,predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    }
  ]
}